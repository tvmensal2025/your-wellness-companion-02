import { ollamaClient } from '../clients/ollamaClient.js';
import { geminiClient } from '../clients/geminiClient.js';

interface UnifiedAssistantInput {
  message: string;
  context?: any;
  userId?: string;
}

export async function unifiedAssistantHandler(input: UnifiedAssistantInput): Promise<any> {
  console.log('ü§ñ Processing unified assistant request...');

  try {
    // Try Ollama first (local, free)
    try {
      const ollamaResult = await ollamaClient.chat(input.message, input.context);
      
      return {
        success: true,
        response: ollamaResult.response,
        model: 'ollama',
        processed_at: new Date().toISOString()
      };
    } catch (ollamaError) {
      console.warn('Ollama failed, falling back to Gemini:', ollamaError);
    }

    // Fallback to Gemini
    const prompt = `Voc√™ √© um assistente de sa√∫de e bem-estar da MaxNutrition.

Mensagem do usu√°rio: ${input.message}

Contexto adicional: ${JSON.stringify(input.context || {})}

Responda de forma clara, amig√°vel e √∫til.`;

    const geminiResult = await geminiClient.chat(prompt);

    return {
      success: true,
      response: geminiResult.response,
      model: 'gemini',
      processed_at: new Date().toISOString()
    };

  } catch (error: any) {
    console.error('Unified assistant handler error:', error);
    throw new Error(`Failed to process message: ${error.message}`);
  }
}
