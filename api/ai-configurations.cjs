const { createClient } = require('@supabase/supabase-js');

// ConfiguraÃ§Ã£o do Supabase
const supabaseUrl = 'https://hlrkoyywjpckdotimtik.supabase.co';
const supabaseServiceKey = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhscmtveXl3anBja2RvdGlrIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTczMjk3Mjk3NCwiZXhwIjoyMDQ4NTQ4OTc0fQ.Ej8Ej8Ej8Ej8Ej8Ej8Ej8Ej8Ej8Ej8Ej8Ej8Ej8Ej8';

const supabase = createClient(supabaseUrl, supabaseServiceKey);

// ConfiguraÃ§Ãµes por preset
const PRESET_CONFIGS = {
  MINIMO: {
    'openai-o3-pro': { max_tokens: 1024, temperature: 0.7 },
    'gpt-4.1': { max_tokens: 1024, temperature: 0.7 },
    'gpt-4.1-mini': { max_tokens: 1024, temperature: 0.7 },
    'gemini-1.5-flash': { max_tokens: 1024, temperature: 0.7 },
    'gemini-1.5-pro': { max_tokens: 1024, temperature: 0.7 }
  },
  MEIO: {
    'openai-o3-pro': { max_tokens: 4096, temperature: 0.8 },
    'gpt-4.1': { max_tokens: 4096, temperature: 0.8 },
    'gpt-4.1-mini': { max_tokens: 4096, temperature: 0.8 },
    'gemini-1.5-flash': { max_tokens: 4096, temperature: 0.8 },
    'gemini-1.5-pro': { max_tokens: 4096, temperature: 0.8 }
  },
  MAXIMO: {
    'openai-o3-pro': { max_tokens: 8192, temperature: 0.8 },
    'gpt-4.1': { max_tokens: 8192, temperature: 0.8 },
    'gpt-4.1-mini': { max_tokens: 8192, temperature: 0.8 },
    'gemini-1.5-flash': { max_tokens: 8192, temperature: 0.8 },
    'gemini-1.5-pro': { max_tokens: 8192, temperature: 0.8 }
  }
};

// Mapeamento de modelos para nomes de funcionalidade
const MODEL_MAPPING = {
  'openai-o3-pro': 'o3-PRO',
  'gpt-4.1': 'gpt-4.1-2025-04-14',
  'gpt-4.1-mini': 'gpt-4.1-mini',
  'gemini-1.5-flash': 'gemini-1.5-flash',
  'gemini-1.5-pro': 'gemini-1.5-pro'
};

// FunÃ§Ã£o para obter configuraÃ§Ãµes de IA
async function getAIConfigurations() {
  try {
    console.log('ğŸ” Buscando configuraÃ§Ãµes de IA...');
    
    const { data, error } = await supabase
      .from('ai_configurations')
      .select('*');

    if (error) {
      console.error('âŒ Erro ao buscar configuraÃ§Ãµes:', error);
      return { success: false, error: error.message };
    }

    console.log('âœ… ConfiguraÃ§Ãµes encontradas:', data?.length || 0);
    return { success: true, data };
  } catch (error) {
    console.error('ğŸ’¥ Erro fatal:', error);
    return { success: false, error: error.message };
  }
}

// FunÃ§Ã£o para atualizar configuraÃ§Ãµes baseada na seleÃ§Ã£o
async function updateConfigurations(selectedModel, selectedPreset) {
  try {
    console.log(`ğŸš€ Atualizando configuraÃ§Ãµes para ${selectedModel} - ${selectedPreset}...`);
    
    // Obter configuraÃ§Ãµes do preset selecionado
    const presetConfig = PRESET_CONFIGS[selectedPreset];
    if (!presetConfig) {
      return { success: false, error: `Preset ${selectedPreset} nÃ£o encontrado` };
    }

    const modelConfig = presetConfig[selectedModel];
    if (!modelConfig) {
      return { success: false, error: `Modelo ${selectedModel} nÃ£o encontrado no preset ${selectedPreset}` };
    }

    // ConfiguraÃ§Ãµes para todas as funcionalidades
    const configurations = [
      {
        functionality: 'chat_daily',
        service: 'openai',
        model: MODEL_MAPPING[selectedModel] || selectedModel,
        max_tokens: modelConfig.max_tokens,
        temperature: modelConfig.temperature,
        is_enabled: true,
        preset_level: selectedPreset.toLowerCase()
      },
      {
        functionality: 'weekly_report',
        service: 'openai',
        model: MODEL_MAPPING[selectedModel] || selectedModel,
        max_tokens: modelConfig.max_tokens,
        temperature: modelConfig.temperature,
        is_enabled: true,
        preset_level: selectedPreset.toLowerCase()
      },
      {
        functionality: 'monthly_report',
        service: 'openai',
        model: MODEL_MAPPING[selectedModel] || selectedModel,
        max_tokens: modelConfig.max_tokens,
        temperature: modelConfig.temperature,
        is_enabled: true,
        preset_level: selectedPreset.toLowerCase()
      },
      {
        functionality: 'medical_analysis',
        service: 'openai',
        model: MODEL_MAPPING[selectedModel] || selectedModel,
        max_tokens: modelConfig.max_tokens,
        temperature: modelConfig.temperature,
        is_enabled: true,
        preset_level: selectedPreset.toLowerCase()
      },
      {
        functionality: 'preventive_analysis',
        service: 'openai',
        model: MODEL_MAPPING[selectedModel] || selectedModel,
        max_tokens: modelConfig.max_tokens,
        temperature: modelConfig.temperature,
        is_enabled: true,
        preset_level: selectedPreset.toLowerCase()
      }
    ];

    // Primeiro, limpar configuraÃ§Ãµes existentes
    const { error: deleteError } = await supabase
      .from('ai_configurations')
      .delete()
      .neq('id', '00000000-0000-0000-0000-000000000000');

    if (deleteError) {
      console.error('âŒ Erro ao limpar configuraÃ§Ãµes:', deleteError);
    } else {
      console.log('âœ… ConfiguraÃ§Ãµes antigas removidas');
    }

    // Inserir novas configuraÃ§Ãµes
    const results = [];
    for (const config of configurations) {
      try {
        const { error } = await supabase
          .from('ai_configurations')
          .insert(config);

        if (error) {
          console.error(`âŒ Erro ao inserir ${config.functionality}:`, error);
          results.push({ functionality: config.functionality, success: false, error: error.message });
        } else {
          console.log(`âœ… ${config.functionality} atualizado`);
          results.push({ functionality: config.functionality, success: true });
        }
      } catch (error) {
        console.error(`ğŸ’¥ Erro fatal ao inserir ${config.functionality}:`, error);
        results.push({ functionality: config.functionality, success: false, error: error.message });
      }
    }

    return { 
      success: true, 
      results,
      config: {
        model: selectedModel,
        preset: selectedPreset,
        max_tokens: modelConfig.max_tokens,
        temperature: modelConfig.temperature
      }
    };
  } catch (error) {
    console.error('ğŸ’¥ Erro fatal:', error);
    return { success: false, error: error.message };
  }
}

// FunÃ§Ã£o para atualizar para configuraÃ§Ã£o mÃ¡xima (mantida para compatibilidade)
async function updateToMaximo() {
  return await updateConfigurations('openai-o3-pro', 'MAXIMO');
}

// FunÃ§Ã£o para testar configuraÃ§Ãµes
async function testAIConfigurations() {
  try {
    console.log('ğŸ§ª Testando configuraÃ§Ãµes de IA...');
    
    const { data, error } = await supabase
      .from('ai_configurations')
      .select('functionality, model, max_tokens, temperature, preset_level');

    if (error) {
      return { success: false, error: error.message };
    }

    console.log('ğŸ“Š ConfiguraÃ§Ãµes atuais:');
    data.forEach(config => {
      console.log(`- ${config.functionality}: ${config.model} (${config.max_tokens} tokens, temp ${config.temperature}, preset ${config.preset_level})`);
    });

    return { success: true, data };
  } catch (error) {
    console.error('ğŸ’¥ Erro fatal:', error);
    return { success: false, error: error.message };
  }
}

// FunÃ§Ã£o para aplicar configuraÃ§Ã£o baseada na seleÃ§Ã£o da interface
async function applyUserSelection(selectedModel, selectedPreset) {
  try {
    console.log(`ğŸ¯ Aplicando seleÃ§Ã£o do usuÃ¡rio: ${selectedModel} - ${selectedPreset}`);
    
    const result = await updateConfigurations(selectedModel, selectedPreset);
    
    if (result.success) {
      console.log('âœ… ConfiguraÃ§Ã£o aplicada com sucesso!');
      console.log(`ğŸ“Š Modelo: ${result.config.model}`);
      console.log(`ğŸ“Š Preset: ${result.config.preset}`);
      console.log(`ğŸ“Š Tokens: ${result.config.max_tokens}`);
      console.log(`ğŸ“Š Temperature: ${result.config.temperature}`);
    } else {
      console.error('âŒ Erro ao aplicar configuraÃ§Ã£o:', result.error);
    }
    
    return result;
  } catch (error) {
    console.error('ğŸ’¥ Erro fatal:', error);
    return { success: false, error: error.message };
  }
}

// Exportar funÃ§Ãµes
module.exports = {
  getAIConfigurations,
  updateToMaximo,
  testAIConfigurations,
  updateConfigurations,
  applyUserSelection,
  PRESET_CONFIGS,
  MODEL_MAPPING
};

// Se executado diretamente
if (require.main === module) {
  async function main() {
    console.log('ğŸ”§ API de ConfiguraÃ§Ãµes de IA - VersÃ£o FlexÃ­vel');
    console.log('='.repeat(60));
    
    // Exemplo de uso com seleÃ§Ã£o do usuÃ¡rio
    const selectedModel = 'openai-o3-pro'; // Modelo selecionado
    const selectedPreset = 'MAXIMO';       // Preset selecionado
    
    console.log(`\nğŸ¯ Aplicando seleÃ§Ã£o: ${selectedModel} - ${selectedPreset}`);
    const result = await applyUserSelection(selectedModel, selectedPreset);
    console.log('Resultado:', result);
    
    // Verificar configuraÃ§Ãµes finais
    console.log('\nğŸ“Š Verificando configuraÃ§Ãµes finais...');
    const finalResult = await testAIConfigurations();
    console.log('Resultado:', finalResult);
  }
  
  main().catch(console.error);
} 